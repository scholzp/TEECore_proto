
use log::info;
use x86::cpuid::CpuId;
use x86::msr::{
	rdmsr, wrmsr,
	MSR_OFFCORE_RSP_0, MSR_OFFCORE_RSP_1,
	IA32_PERFEVTSEL0, IA32_PMC0,
	IA32_PERFEVTSEL1, IA32_PMC1,
	IA32_PERFEVTSEL2, IA32_PMC2,
	IA32_PERFEVTSEL3, IA32_PMC3,
	IA32_PERFEVTSEL4, IA32_PMC4,
	IA32_PERFEVTSEL5, IA32_PMC5,
	IA32_PERFEVTSEL6, IA32_PMC6,
	IA32_PERFEVTSEL7, IA32_PMC7,
};
// use x86_64::instructions::nop;
use alloc::vec::Vec;
use crate::pmc_utils::vendor;

/// https://perfmon-events.intel.com/
/// Supported architectures: Skylake

/// Bitmask to select MSR_OFFCORE_RSP0 as offcore event configuration provider
pub const OFFCORE_RSP0_EVENT_CODE: u64 = 0xb7;
/// Bitmask to select MSR_OFFCORE_RSP1 as offcore event configuration provider
pub const OFFCORE_RSP1_EVENT_CODE: u64 = 0xbb;
/// UMASK for offcore events
pub const OFFCORE_RSP_UNIT_MASK: u64 = 0x01 << 8;

/// Counts the number of demand data reads and page table entry cacheline reads.
/// Does not count hw or sw prefetches.
pub const REQUEST_DMND_DATA_RD: u64 = 0x1 << 0;
/// Counts the number of demand reads for ownership (RFO) requests generated by
/// a write to data cacheline. Does not count L2 RFO prefetches.
pub const REQUEST_DMND_RFO: u64 = 0x1 << 1;
pub const REQUEST_DMND_CODE_RD: u64 = 0x1 << 2;
/// Counts the number of demand instruction cacheline reads and L1 instruction
/// cacheline prefetches.
pub const REQUEST_DMND_HWPF_L2_DATA_RD: u64 = 0x1 << 4;
pub const REQUEST_DMND_HWPF_L2_RFO: u64 = 0x1 << 5;
pub const REQUEST_DMND_HWPF_L3: u64 = (0x1 << 7) | (0x1 << 8) | (0x1 << 9) | (0x1 << 13);
pub const REQUEST_DMND_HWPF_L1D_AND_SWPF: u64 = 0x1 << 10;
pub const REQUEST_DMND_STREAMING_WR: u64 = 0x1 << 11;


/// Counts miscellaneous requests, such as I/O and uncacheable accesses.
pub const REQUEST_OTHER: u64 = 0x1 << 15;

/// Catch all value for any response types.
pub const SUPPLIER_ANY: u64 = 0x1 << 16;
/// No Supplier Information available.
pub const SUPPLIER_NO_SUPP: u64 = 0x1 << 17;
/// M-state initial lookup stat in L3.
pub const SUPPLIER_L3_HITM_SATE: u64 = 0x1 << 18;
/// E-state initial lookup stat in L3.
pub const SUPPLIER_L3_HIT_E_SATE: u64 = 0x1 << 19;
/// S-state initial lookup stat in L3.
pub const SUPPLIER_L3_HIT_S_SATE: u64 = 0x1 << 20;
/// L4 Cache hit
pub const SUPPLIER_L4_HIT: u64 = 0x1 << 22;
/// Local Node
pub const SUPPLIER_DRAM: u64 = 0x1 << 26;
/// L4 cache super line hit (if L4 present)
pub const SUPPLIER_SPL_HIT: u64 = 0x1 << 30;

/// No details on snoop-related information.
pub const SNOOP_NONE: u64 = 0x1 << 31;
/// No snoop was needed to satisfy the request.
pub const SNOOP_NOT_NEEDED: u64 = 0x1 << 32;
/// A snoop was needed and it missed all snooped caches
pub const SNOOP_MISS: u64 = 0x1 << 33;
/// A snoop was needed and it hits in at least one snooped cache. Hit denotes a
/// cache-line was valid before snoop effect.
pub const SNOOP_HIT_NO_FWD: u64 = 0x1 << 34;
/// A snoop was needed and data was forwarded from a remote socket.
pub const SNOOP_HIT_WITH_FWD: u64 = 0x1 << 35;
/// A snoop was needed and it HitM-ed in local or remote cache. HitM denotes a
/// cache-line was in modified state before effect as a results of snoop.
pub const SNOOP_HITM: u64 = 0x1 << 36;
/// Target was non-DRAM system address. This includes MMIO transactions.
pub const SNOOP_NO_DRAM: u64 = 0x1 << 37;

/// USR bit in PERFEVTSEL. When set, counter is incremented when logical core is
/// in privilege level 1,2 or 3.
pub const  IA32_PERFEVTSEL_USR: u64 = 0x1 << 16;
/// OS bit of PERFEVTSEL. When set, counter is incremented when logical core is
/// in privilege level 0.
pub const  IA32_PERFEVTSEL_OS: u64 = 0x1 << 17;
/// E bit in PERFEVTSEL. Enables (when set) edge detection of the selected
/// microarchitectural condition.
pub const  IA32_PERFEVTSEL_E: u64 = 0x1 << 18;
/// PC bit in PERFEVTSEL. Not supported since Sandy Bridge (Core 2xxx). When set
/// processor toggles PMi pins and increments the PMC. When clear, processor
/// toggles PMi pins on counter overflow
pub const  IA32_PERFEVTSEL_PC: u64 = 0x1 << 19;
/// When set, the logical processor generates an exception through its local
/// APIC on counter overflow
pub const  IA32_PERFEVTSEL_INT: u64 = 0x1 << 20;
/// When set the corresponding PMC counts the event. When clear, the counting
/// stops and the corresponding PMC can be written
pub const  IA32_PERFEVTSEL_EN: u64 = 0x1 << 22;
/// Invert flag. Inverts counter mask when set.
pub const  IA32_PERFEVTSEL_INV: u64 = 0x1 << 23;

/// Contains bits enabling the events counting Snoop responses Multiple bits can
/// be set to true. Occupies bit 31..37 in MSR_OFFCORE_RSP_x.
pub struct MsrOffcoreRspEventCounter {
	index: u8,
	pmc_index: u8,
	content: u64,
}

impl Default for MsrOffcoreRspEventCounter {
	fn default() -> Self {
		Self {
			index: 0x0_u8,
			pmc_index: 0x0_u8,
			content: 0x0_u64,
		}
	}
}

impl MsrOffcoreRspEventCounter {
	/// Creates new MsrOffcoreRspConfig with given id.
	///
	/// A processor can implement multiple MSR_OFFCORE_RSP registers. In this
	/// case they are denoted MSR_OFFCORE_RSP_x in the Intel SDM.
	///
	/// * `x`			- Index of the MSR_OFFCORE_RSP to use
	/// * `pmc_index`	- Index of the GP performance monitoring register to use
	pub fn new(x: u8, pmc_index: u8) -> Self {
		Self {
			index: x,
			pmc_index: pmc_index,
			content: 0x0_u64,
		}
	}

	/// Updates the configuration stored in this struct.
	///
	/// This does not automatically write to the respective MSR
	///
	/// * `config`- Bitvector to use for later operations
	pub fn set_offcore_configuration(&mut self, config: u64) {
		self.content = config;
	}

	/// Sets index.
	///
	/// * `x`- Index of the MSR_OFFCORE_RSP to use
	pub fn set_index(&mut self, x: u8) {
		self.index = x;
	}

	/// Initialize and activate the counter facility.
	///
	/// Write the configuration to the MSR_OFFCORE_RSP and activate the
	/// respective GP PMC to count events using this configuration. Reset the
	/// counter to the given value.
	///
	///
	/// * `init_v`: Value to reset the counter to
	pub fn activate_counter(&self, init_v: u64) {
		/* To activate a offcore PMC, we need to do the following things:
		*  1) Configure the MSR_OFFCORE_RSPx with the actual event configuration
		*  2) Configure the IA32_PERFEVTSELx with the behavior we wish for
		*  3) Event and UMASK in IA32_PERFEVTSELx are chosen so that the
		*	 PMC uses the configuration from MSR_OFFCORE_RSPx
		*  4) Initialize IA32_PMCx (do we increment, do we decrement...?)
		*  5) Start the counter by setting the bit in IA32_PERFEVTSELx
		*/
		let mut event_code: u64 = 0x0;
		let mut msr_pmc: u32 = 0;
		let mut msr_pmc_eventsel: u32 = 0;
		// 1) Find the MSR_OFFCORE_RSPx to use and write the configuration to
		//	it
		match self.index {
			0 => unsafe {
				wrmsr(MSR_OFFCORE_RSP_0, self.content);
				event_code = OFFCORE_RSP0_EVENT_CODE;
			},
			1 => unsafe {
				wrmsr(MSR_OFFCORE_RSP_1, self.content);
				event_code = OFFCORE_RSP1_EVENT_CODE;
			},
			_ => return, //TODO: We want, at some point, return an error
		}
		// We cant to count all occurences (OS and User) of the eventcode of the
		// chosen OFFCORE_RSP
		let perfsel_content = 0x0_u64
		| IA32_PERFEVTSEL_USR | IA32_PERFEVTSEL_OS // count in all priv levels
		| event_code			// Event depending on chosen MSR_OFFCORE_RSPx
		| OFFCORE_RSP_UNIT_MASK // offcore event UMASK
		| IA32_PERFEVTSEL_EN; 	// Start the counter
		// 2 & 3 & 4 & 5) Depending on the index of the PMC we do the same thing

		match self.pmc_index {
			0 => {
				msr_pmc = IA32_PMC0;
				msr_pmc_eventsel = IA32_PERFEVTSEL0;
			},
			1 => {
				// We operate on PMC1 and IA32_PERFEVTSEL1
				msr_pmc = IA32_PMC1;
				msr_pmc_eventsel = IA32_PERFEVTSEL1;
			},
			2 => {
				// We operate on PMC2 and IA32_PERFEVTSEL2
				msr_pmc = IA32_PMC2;
				msr_pmc_eventsel = IA32_PERFEVTSEL2;

			},
			3 => {
				// We operate on PMC3 and IA32_PERFEVTSEL3
				msr_pmc = IA32_PMC3;
				msr_pmc_eventsel = IA32_PERFEVTSEL3;
			},
			4 => {
				// We operate on PMC3 and IA32_PERFEVTSEL3
				msr_pmc = IA32_PMC4;
				msr_pmc_eventsel = IA32_PERFEVTSEL4;
			},
			5 => {
				// We operate on PMC3 and IA32_PERFEVTSEL3
				msr_pmc = IA32_PMC5;
				msr_pmc_eventsel = IA32_PERFEVTSEL5;
			},
			6 => {
				// We operate on PMC3 and IA32_PERFEVTSEL3
				msr_pmc = IA32_PMC6;
				msr_pmc_eventsel = IA32_PERFEVTSEL6;
			},
			7 => {
				// We operate on PMC3 and IA32_PERFEVTSEL3
				msr_pmc = IA32_PMC7;
				msr_pmc_eventsel = IA32_PERFEVTSEL7;
			},
			_ => {
				info!("No CPU known to implement 8 or more GP PMCs!");
				return;  //TODO: We want, at some point, return an error
			},
		}
		Self::init_and_conf_pmc(
			msr_pmc_eventsel, msr_pmc, init_v, perfsel_content
		);
	}

	fn init_and_conf_pmc(perfevtsel_register: u32, pmc_register: u32, init_v: u64, perfsel_content: u64) {
		unsafe {
			// Cancel any running performance measurements
			wrmsr(perfevtsel_register, 0x0_u64);
			// Reset the counter to zero
			wrmsr(pmc_register, init_v);
			// MSR_OFFCOREx was configured before
			// Activate the counter
			wrmsr(perfevtsel_register, perfsel_content);
		}
	}

	pub fn read_pcm_val(&self) -> u64 {
		match self.pmc_index {
			0 => unsafe { rdmsr(IA32_PMC0) },
			1 => unsafe { rdmsr(IA32_PMC1) },
			2 => unsafe { rdmsr(IA32_PMC2) },
			3 => unsafe { rdmsr(IA32_PMC3) },
			4 => unsafe { rdmsr(IA32_PMC4) },
			5 => unsafe { rdmsr(IA32_PMC5) },
			6 => unsafe { rdmsr(IA32_PMC6) },
			7 => unsafe { rdmsr(IA32_PMC7) },
			_ => {
				info!("No CPU known to implement 8 or more GP PMCs!");
				//return;  //TODO: We want, at some point, return an error
				0
			},
		}
	}
}

/// Returns performance monitoring related features of th CPU
pub fn query_features_intel() {
	if false == vendor::check_vendor(vendor::CpuVendor::Intel) {
		return
	}
	let cpuid = CpuId::new();
	info!("{:?}", cpuid.get_performance_monitoring_info().unwrap());
}
